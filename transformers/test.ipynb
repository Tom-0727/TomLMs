{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding\n",
    "- 在nn.Module中，register_buffer()是一个方法，用于在PyTorch模型中注册一个缓冲区（buffer）。缓冲区是一种状态，不同于模型的参数，它们不会被优化，但可以在模型中使用。通常，缓冲区用于存储与模型相关的不可训练数据，例如在BatchNormalization中使用的运行统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_model_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, embed_model_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        pe = pe.unsqueeze(0)  # add a new dimension of size 1 at the pos 0\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Add constant to embedding\n",
    "        seq_len = x.size(1)  # get the size of dim=1\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "- with mask mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int = 512, \n",
    "                 n_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Basic Attributes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dk = embed_dim // n_heads\n",
    "\n",
    "        # Query, Key, Value : input_dim = d_model//n_heads = dk\n",
    "        self.Q = nn.Linear(self.dk, self.dk, bias=False)\n",
    "        self.K = nn.Linear(self.dk, self.dk, bias=False)\n",
    "        self.V = nn.Linear(self.dk, self.dk, bias=False)\n",
    "        self.out = nn.Linear(self.n_heads * self.dk, self.embed_dim)\n",
    "\n",
    "    def forward(self, key, query, value, mask = None):\n",
    "        \n",
    "        # Get dim info\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "\n",
    "        # query dimension could change in decoder during inference\n",
    "        seq_length_query = query.size(1)\n",
    "\n",
    "        # (batch_size x seq_length x 8 x 64)\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.dk)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.dk)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.dk)\n",
    "\n",
    "        k = self.K(key)\n",
    "        q = self.Q(query)\n",
    "        v = self.V(value)\n",
    "\n",
    "        # (batch_size, n_heads, seq_len, dk)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # computes attention\n",
    "        k_T = k.transpose(-1, -2)  # (batch_size, n_heads, dk, seq_len)\n",
    "        product = torch.matmul(q, k_T)/math.sqrt(self.dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            product = product.masked_fill(mask == 0, float(-1e20))\n",
    "\n",
    "        scores = torch.matmul(F.softmax(product, dim=-1), v)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        # (32x8x10x64) -> (32x10x8x64)  -> (batch_size, seq_len, d_model)\n",
    "        concat = scores.transpose(1, 2).contiguous().view(batch_size, seq_length_query, self.dk*self.n_heads)\n",
    "\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(EncodeBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion_factor),  # 512 * 2048\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * expansion_factor, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, key, query, value):\n",
    "        attention_out = self.attention(key, query, value)\n",
    "        attention_res_out = attention_out + value\n",
    "        norm1_out = self.dropout1(self.norm1(attention_res_out))\n",
    "\n",
    "        ff_out = self.feed_forward(norm1_out)\n",
    "        ff_res_out = ff_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(ff_res_out))\n",
    "\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=6, expansion_factor=4, n_heads=8):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncodeBlock(embed_dim, expansion_factor, n_heads) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
