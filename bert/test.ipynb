{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(2,3,4)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_class = nn.Linear(4, 2)\n",
    "k_class.weight.data = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)\n",
    "k_class.bias.data = torch.tensor([1, 2], dtype=torch.float32)\n",
    "\n",
    "opt = k_class(k[:, 1:,:])\n",
    "print(opt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbone import *\n",
    "\n",
    "model = BERT(embed_dim = 768,\n",
    "                 num_heads = 12,\n",
    "                 expansion_factor = 4,\n",
    "                 num_layers = 12,\n",
    "                 max_seq_len = 256,\n",
    "                 vocab_size = 12000)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DownLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DownLoad\n",
    "from modules import *\n",
    "\n",
    "bookscorpus_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "\n",
    "corpus_pth = './data/books_test'\n",
    "\n",
    "txt_clean(corpus_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')\n",
    "sys.path.append('../tokenization/tools/')\n",
    "\n",
    "from tokenization.tokenizer import *\n",
    "from tokenization.tools import *\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load('../transformers/iwslt2013_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Produce_data_pair_corpus(corpus_path, tokenizer, max_seq_len=256):\n",
    "    file_names = os.listdir(corpus_path)\n",
    "    data_pair_corpus = []\n",
    "    for file_name in file_names:\n",
    "        print('Processing file: ', file_name)\n",
    "        with open(os.path.join(corpus_path, file_name), 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = tokenizer.tokenize(line)[0]\n",
    "            num_token = len(tokens)\n",
    "            num_pair = num_token // max_seq_len + 1\n",
    "            for i in range(num_pair):\n",
    "                if i == num_pair - 1:\n",
    "                    percent = random.uniform(0.4, 0.6)\n",
    "                    index = int(percent * (num_token - i * max_seq_len))\n",
    "                    data_pair_corpus.append([tokens[i*max_seq_len:i*max_seq_len+index], tokens[i*max_seq_len+index:]])\n",
    "                else:\n",
    "                    percent = random.uniform(0.4, 0.6)\n",
    "                    index = int(percent * max_seq_len)\n",
    "                    data_pair_corpus.append((tokens[i*max_seq_len:i*max_seq_len+index], tokens[i*max_seq_len+index:(i+1)*max_seq_len]))\n",
    "\n",
    "    return data_pair_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pair = produce_data_pair_corpus(\n",
    "    corpus_path = './data/books_test/',\n",
    "    tokenizer = tokenizer,\n",
    "    max_seq_len = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in data_pair:\n",
    "    print(len(pair[0]), len(pair[1]))\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_pth = './data/books_test/data_pair_corpus.pkl'\n",
    "\n",
    "with open(file_pth, 'wb') as f:\n",
    "    pickle.dump(data_pair, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from modules import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')\n",
    "sys.path.append('../tokenization/tools/')\n",
    "\n",
    "from tokenization.tokenizer import *\n",
    "from tokenization.tools import *\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load('../transformers/iwslt2013_tokenizer.pkl')\n",
    "\n",
    "import pickle\n",
    "\n",
    "file_pth = './data/books_test/data_pair_corpus.pkl'\n",
    "\n",
    "with open(file_pth, 'rb') as f:\n",
    "    data_pair_corpus = pickle.load(f)\n",
    "\n",
    "\n",
    "train_dataset = BERTDataset(\n",
    "    data_pair_corpus = data_pair_corpus,\n",
    "    tokenizer = tokenizer,\n",
    "    mask_token = '[MASK]',\n",
    "    cls_token = '[CLS]',\n",
    "    pad_token = '[PAD]',\n",
    "    seq_token = '[SEP]',\n",
    "    max_seq_len = 256\n",
    ")\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 8,\n",
    "    shuffle = True,\n",
    "    num_workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for item in dataloader:\n",
    "    i -= 1\n",
    "    if i == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
