{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DownLoad to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def get_iwsltenvi_data(store_dir: str = './'):\n",
    "    train_en_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en'\n",
    "    train_vi_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi'\n",
    "    test_en_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en'\n",
    "    test_vi_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi'\n",
    "\n",
    "    store_pth = f'{store_dir}data/iwsltenvi/'\n",
    "    os.makedirs(store_pth, exist_ok=True)\n",
    "    print('The Data would be stored in: ', store_pth)\n",
    "\n",
    "    if not os.path.exists(f'{store_pth}train_en.txt'):\n",
    "        train_en = requests.get(train_en_url).text\n",
    "        with open(f'{store_pth}train_en.txt', 'w') as f:\n",
    "            f.write(train_en)\n",
    "    \n",
    "    if not os.path.exists(f'{store_pth}train_vi.txt'):\n",
    "        train_vi = requests.get(train_vi_url).text\n",
    "        with open(f'{store_pth}train_vi.txt', 'w') as f:\n",
    "            f.write(train_vi)\n",
    "    \n",
    "    if not os.path.exists(f'{store_pth}test_en.txt'):\n",
    "        test_en = requests.get(test_en_url).text\n",
    "        with open(f'{store_pth}test_en.txt', 'w') as f:\n",
    "            f.write(test_en)\n",
    "    \n",
    "    if not os.path.exists(f'{store_pth}test_vi.txt'):\n",
    "        test_vi = requests.get(test_vi_url).text\n",
    "        with open(f'{store_pth}test_vi.txt', 'w') as f:\n",
    "            f.write(test_vi)\n",
    "    \n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data would be stored in:  ./data/iwsltenvi/\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "get_iwsltenvi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .txt and get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# Load & Clean the data (Convert HTML-encoded characters to normal)\n",
    "def load_iwsltenvi_data(train: bool = True,\n",
    "                        test: bool = True,\n",
    "                        data_dir: str = './data/iwsltenvi/'):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    # Load the data\n",
    "    if train:\n",
    "        with open(f'{data_dir}train_en.txt', 'r') as f:\n",
    "            en_text = html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}train_vi.txt', 'r') as f:\n",
    "            vi_text = html.unescape(f.read()).split('\\n')\n",
    "        train_data['en'] = en_text\n",
    "        train_data['vi'] = vi_text\n",
    "        \n",
    "    if test:\n",
    "        with open(f'{data_dir}test_en.txt', 'r') as f:\n",
    "            en_text += html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}test_vi.txt', 'r') as f:\n",
    "            vi_text += html.unescape(f.read()).split('\\n')\n",
    "\n",
    "        test_data['en'] = en_text\n",
    "        test_data['vi'] = vi_text\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_iwsltenvi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Corpus\n",
    "corpus = []\n",
    "corpus.extend(train_data['en'])\n",
    "corpus.extend(train_data['vi'])\n",
    "corpus.extend(test_data['en'])\n",
    "corpus.extend(test_data['vi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')\n",
    "\n",
    "import html\n",
    "\n",
    "from tokenization.tools import *\n",
    "from tokenization.tokenizer import *\n",
    "\n",
    "# Load & Clean the data (Convert HTML-encoded characters to normal)\n",
    "def load_iwsltenvi_data(train: bool = True,\n",
    "                        test: bool = True,\n",
    "                        data_dir: str = './data/iwsltenvi/'):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    # Load the data\n",
    "    if train:\n",
    "        with open(f'{data_dir}train_en.txt', 'r') as f:\n",
    "            en_text = html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}train_vi.txt', 'r') as f:\n",
    "            vi_text = html.unescape(f.read()).split('\\n')\n",
    "        train_data['en'] = en_text\n",
    "        train_data['vi'] = vi_text\n",
    "        \n",
    "    if test:\n",
    "        with open(f'{data_dir}test_en.txt', 'r') as f:\n",
    "            en_text += html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}test_vi.txt', 'r') as f:\n",
    "            vi_text += html.unescape(f.read()).split('\\n')\n",
    "\n",
    "        test_data['en'] = en_text\n",
    "        test_data['vi'] = vi_text\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_iwsltenvi_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = BPETokenizer(corpus, 16000)\n",
    "bpe_tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "bpe_tokenizer.save(\n",
    "    name = 'iwslt2013_tokenizer',\n",
    "    dir_pth = './' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "bpe_tokenizer = BPETokenizer()\n",
    "bpe_tokenizer.load(file_path = './iwslt2013_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.pad_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')\n",
    "\n",
    "import html\n",
    "\n",
    "from tokenization.tools import *\n",
    "from tokenization.tokenizer import *\n",
    "\n",
    "# Load & Clean the data (Convert HTML-encoded characters to normal)\n",
    "def load_iwsltenvi_data(train: bool = True,\n",
    "                        test: bool = True,\n",
    "                        data_dir: str = './data/iwsltenvi/'):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    # Load the data\n",
    "    if train:\n",
    "        with open(f'{data_dir}train_en.txt', 'r') as f:\n",
    "            en_text = html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}train_vi.txt', 'r') as f:\n",
    "            vi_text = html.unescape(f.read()).split('\\n')\n",
    "        train_data['en'] = en_text\n",
    "        train_data['vi'] = vi_text\n",
    "        \n",
    "    if test:\n",
    "        with open(f'{data_dir}test_en.txt', 'r') as f:\n",
    "            en_text += html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}test_vi.txt', 'r') as f:\n",
    "            vi_text += html.unescape(f.read()).split('\\n')\n",
    "\n",
    "        test_data['en'] = en_text\n",
    "        test_data['vi'] = vi_text\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_iwsltenvi_data()\n",
    "\n",
    "# Load\n",
    "bpe_tokenizer = BPETokenizer()\n",
    "bpe_tokenizer.load(file_path = './iwslt2013_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "pin_memory = True\n",
    "\n",
    "train_dataset = IWSLTDataset(tokenizer = bpe_tokenizer, \n",
    "                             data = train_data)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: Type:  <class 'list'> Shape:  8\n",
      "trg: Type:  <class 'list'> Shape:  8\n",
      "Maybe we don 't all quite move the same way .\n",
      "Có thể là chúng ta không hoàn toàn di chuyển theo cùng một cách .\n",
      "So the first thing we did was , we compared , what 's different in the brain of someone with depression and someone who is normal , and what we did was PET scans to look at the blood flow of the brain , and what we noticed is that in patients with depression compared to normals , areas of the brain are shut down , and those are the areas in blue .\n",
      "Vậy điều đầu tiên chúng tôi làm được là chúng tôi so sánh sự khác nhau trong não bộ của người trầm cảm với não bộ của người khoẻ mạnh , và điều mà chúng tôi làm là chụp cắt lớp phát xạ để xem dòng chảy máu của não bộ , và điều mà chúng tôi nhận ra là não bệnh nhân trầm cảm khi so sánh với não bộ người khoẻ mạnh thì các vùng trên não đóng lại , và những vùng đó được biểu thị bằng màu xanh .\n",
      "I kind of think of it as this cartoon devil or angel sitting on our shoulders .\n",
      "Tôi nghĩ về điều đó như kiểu hoạt hình thiên thần và ác quỷ ngồi trên hai vai ta .\n",
      "What is a sexy body ?\n",
      "Một cơ thể hấp dẫn phải ra sao ?\n",
      "Music transcends the aesthetic beauty alone .\n",
      "Âm nhạc còn vượt qua cả vẻ đẹp đã được thẩm mỹ nói riêng .\n",
      "H We are doing C-sections and different operations because people need some help .\n",
      "H Chúng tôi thực hiện sinh mổ và một vài loại phẫu thuật khác bởi vì mọi người cần được giúp đỡ .\n",
      "I 've always had stage fright , and not just a little bit , it 's a big bit .\n",
      "Tôi luôn bị sợ sân khấu và không phải chỉ một chút mà là cực kì sợ .\n",
      "Today , we use a single feedstock , petroleum , to heat our homes , power our cars and make most of the materials you see around you .\n",
      "Ngày nay chúng ta chỉ xài 1 loại nguyên liệu thô duy nhất đó là dầu hoả , để sưởi ấm , làm nhiên liệu cho xe cộ và để sản xuất ra những loại vật liệu quen thuộc khác .\n"
     ]
    }
   ],
   "source": [
    "batch = next(data_iter)\n",
    "src, trg = batch\n",
    "print('src: Type: ', str(type(src)), 'Shape: ', str(len(src)))\n",
    "print('trg: Type: ', str(type(trg)), 'Shape: ', str(len(trg)))\n",
    "for i in range(len(src)):\n",
    "    print(src[i])\n",
    "    print(trg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config a Model\n",
    "from transformer import *\n",
    "\n",
    "model = Transformer(embed_dim = 512, \n",
    "                    s_vocab_size = 16001, \n",
    "                    t_vocab_size = 16001, \n",
    "                    max_seq_len = 256, \n",
    "                    num_layers = 6, \n",
    "                    expansion_factor = 4,\n",
    "                    n_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Padding with max_len\n",
    "def dynamic_padding(batch, \n",
    "                    pad_id: int = 0, \n",
    "                    max_seq_len: int = 256):\n",
    "    # Find the longest sequence in the batch\n",
    "    max_len = min(max_seq_len, max([len(x) for x in batch]))\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        # Truncate the sequence > max_len\n",
    "        batch[i] = batch[i][:max_len]\n",
    "\n",
    "        # Fill the rest of the sequence with padding\n",
    "        batch[i] = batch[i] + [pad_id] * (max_len - len(batch[i]))\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "in_d_tensor = torch.tensor([[1, 22, 3], [14, 5, 6]])\n",
    "\n",
    "opt = model(in_tensor, in_d_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(opt.view(-1, opt.size(-1)), in_d_tensor.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6804, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
