{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DownLoad to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def get_iwsltenvi_data(store_dir: str = './'):\n",
    "    train_en_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en'\n",
    "    train_vi_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi'\n",
    "    test_en_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en'\n",
    "    test_vi_url = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi'\n",
    "\n",
    "    store_pth = f'{store_dir}data/iwsltenvi/'\n",
    "    os.makedirs(store_pth, exist_ok=True)\n",
    "    print('The Data would be stored in: ', store_pth)\n",
    "\n",
    "    if not os.path.exists(f'{store_pth}train_en.txt'):\n",
    "        train_en = requests.get(train_en_url).text\n",
    "        with open(f'{store_pth}train_en.txt', 'w') as f:\n",
    "            f.write(train_en)\n",
    "    \n",
    "    if not os.path.exists(f'{store_pth}train_vi.txt'):\n",
    "        train_vi = requests.get(train_vi_url).text\n",
    "        with open(f'{store_pth}train_vi.txt', 'w') as f:\n",
    "            f.write(train_vi)\n",
    "    \n",
    "    if not os.path.exists(f'{store_pth}test_en.txt'):\n",
    "        test_en = requests.get(test_en_url).text\n",
    "        with open(f'{store_pth}test_en.txt', 'w') as f:\n",
    "            f.write(test_en)\n",
    "    \n",
    "    if not os.path.exists(f'{store_pth}test_vi.txt'):\n",
    "        test_vi = requests.get(test_vi_url).text\n",
    "        with open(f'{store_pth}test_vi.txt', 'w') as f:\n",
    "            f.write(test_vi)\n",
    "    \n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iwsltenvi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .txt and get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# Load & Clean the data (Convert HTML-encoded characters to normal)\n",
    "def load_iwsltenvi_data(train: bool = True,\n",
    "                        test: bool = True,\n",
    "                        data_dir: str = './data/iwsltenvi/'):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    # Load the data\n",
    "    if train:\n",
    "        with open(f'{data_dir}train_en.txt', 'r') as f:\n",
    "            en_text = html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}train_vi.txt', 'r') as f:\n",
    "            vi_text = html.unescape(f.read()).split('\\n')\n",
    "        train_data['en'] = en_text\n",
    "        train_data['vi'] = vi_text\n",
    "        \n",
    "    if test:\n",
    "        with open(f'{data_dir}test_en.txt', 'r') as f:\n",
    "            en_text += html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}test_vi.txt', 'r') as f:\n",
    "            vi_text += html.unescape(f.read()).split('\\n')\n",
    "\n",
    "        test_data['en'] = en_text\n",
    "        test_data['vi'] = vi_text\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_iwsltenvi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Corpus\n",
    "corpus = []\n",
    "corpus.extend(train_data['en'])\n",
    "corpus.extend(train_data['vi'])\n",
    "corpus.extend(test_data['en'])\n",
    "corpus.extend(test_data['vi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')\n",
    "\n",
    "import html\n",
    "\n",
    "from tokenization.tools import *\n",
    "from tokenization.tokenizer import *\n",
    "\n",
    "# Load & Clean the data (Convert HTML-encoded characters to normal)\n",
    "def load_iwsltenvi_data(train: bool = True,\n",
    "                        test: bool = True,\n",
    "                        data_dir: str = './data/iwsltenvi/'):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    # Load the data\n",
    "    if train:\n",
    "        with open(f'{data_dir}train_en.txt', 'r') as f:\n",
    "            en_text = html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}train_vi.txt', 'r') as f:\n",
    "            vi_text = html.unescape(f.read()).split('\\n')\n",
    "        train_data['en'] = en_text\n",
    "        train_data['vi'] = vi_text\n",
    "        \n",
    "    if test:\n",
    "        with open(f'{data_dir}test_en.txt', 'r') as f:\n",
    "            en_text += html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}test_vi.txt', 'r') as f:\n",
    "            vi_text += html.unescape(f.read()).split('\\n')\n",
    "\n",
    "        test_data['en'] = en_text\n",
    "        test_data['vi'] = vi_text\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_iwsltenvi_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = BPETokenizer(corpus, 16000)\n",
    "bpe_tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "bpe_tokenizer.save(\n",
    "    name = 'iwslt2013_tokenizer',\n",
    "    dir_pth = './' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "bpe_tokenizer = BPETokenizer()\n",
    "bpe_tokenizer.load(file_path = './iwslt2013_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.pad_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../tokenization')\n",
    "\n",
    "import html\n",
    "\n",
    "from tokenization.tools import *\n",
    "from tokenization.tokenizer import *\n",
    "\n",
    "# Load & Clean the data (Convert HTML-encoded characters to normal)\n",
    "def load_iwsltenvi_data(train: bool = True,\n",
    "                        test: bool = True,\n",
    "                        data_dir: str = './data/iwsltenvi/'):\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    # Load the data\n",
    "    if train:\n",
    "        with open(f'{data_dir}train_en.txt', 'r') as f:\n",
    "            en_text = html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}train_vi.txt', 'r') as f:\n",
    "            vi_text = html.unescape(f.read()).split('\\n')\n",
    "        train_data['en'] = en_text\n",
    "        train_data['vi'] = vi_text\n",
    "        \n",
    "    if test:\n",
    "        with open(f'{data_dir}test_en.txt', 'r') as f:\n",
    "            en_text += html.unescape(f.read()).split('\\n')\n",
    "        with open(f'{data_dir}test_vi.txt', 'r') as f:\n",
    "            vi_text += html.unescape(f.read()).split('\\n')\n",
    "\n",
    "        test_data['en'] = en_text\n",
    "        test_data['vi'] = vi_text\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_iwsltenvi_data()\n",
    "\n",
    "# Load\n",
    "bpe_tokenizer = BPETokenizer()\n",
    "bpe_tokenizer.load(file_path = './iwslt2013_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "pin_memory = True\n",
    "\n",
    "train_dataset = IWSLTDataset(tokenizer = bpe_tokenizer, \n",
    "                             data = train_data)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory = pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(data_iter)\n",
    "src, trg = batch\n",
    "print('src: Type: ', str(type(src)), 'Shape: ', str(len(src)))\n",
    "print('trg: Type: ', str(type(trg)), 'Shape: ', str(len(trg)))\n",
    "for i in range(len(src)):\n",
    "    print(src[i])\n",
    "    print(trg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config a Model\n",
    "from transformer import *\n",
    "\n",
    "model = Transformer(embed_dim = 512, \n",
    "                    s_vocab_size = 16001, \n",
    "                    t_vocab_size = 16001, \n",
    "                    max_seq_len = 256, \n",
    "                    num_layers = 6, \n",
    "                    expansion_factor = 4,\n",
    "                    n_heads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Padding with max_len\n",
    "def dynamic_padding(batch, \n",
    "                    pad_id: int = 0, \n",
    "                    max_seq_len: int = 256):\n",
    "    # Find the longest sequence in the batch\n",
    "    max_len = min(max_seq_len, max([len(x) for x in batch]))\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        # Truncate the sequence > max_len\n",
    "        batch[i] = batch[i][:max_len]\n",
    "\n",
    "        # Fill the rest of the sequence with padding\n",
    "        batch[i] = batch[i] + [pad_id] * (max_len - len(batch[i]))\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "in_d_tensor = torch.tensor([[1, 22, 3], [14, 5, 6]])\n",
    "\n",
    "opt = model(in_tensor, in_d_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(opt.view(-1, opt.size(-1)), in_d_tensor.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the current device\n",
    "current_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a tensor 'x'\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x = x.to(current_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
